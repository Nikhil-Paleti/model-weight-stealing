# -*- coding: utf-8 -*-
"""DisGUIDE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E4cIhcX-on_N3ZOpPOCCNFCAYMZexAWD
"""

# pip install transformers datasets scikit-learn numpy

import json, os, random, sys, time
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.multioutput import MultiOutputRegressor
from sklearn.linear_model import Ridge, SGDRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from joblib import dump

# ================= CONFIGURATION =================
# We replaced argparse with these variables for Colab ease
TEACHER_MODEL_NAME = "textattack/bert-base-uncased-yelp-polarity"
DATASET_NAME = "yelp_polarity"  # We use the text from here
QUERY_BUDGET = 1000             # Total samples to steal
INITIAL_SIZE = 100              # Random seed set size
STEP_SIZE = 100                 # How many to query per round
VAL_SIZE = 1000                 # Validation set size
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Running on: {DEVICE}")
# =================================================

# ==== 1. Helper: Live Teacher Wrapper ====
class LiveTeacher:
    def __init__(self, model_name: str, device: str = "cpu", batch_size: int = 32):
        print(f"[teacher] Loading {model_name} on {device}...")
        self.device = device
        self.batch_size = batch_size
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(self.device)
        self.model.eval()

    def predict(self, texts: list) -> np.ndarray:
        """Returns probabilities [N, C] for a list of texts."""
        all_probs = []

        # Batch inference for speed
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i : i + self.batch_size]
            # Tokenize
            inputs = self.tokenizer(
                batch, padding=True, truncation=True, max_length=128, return_tensors="pt"
            ).to(self.device)

            with torch.no_grad():
                logits = self.model(**inputs).logits
                probs = torch.softmax(logits, dim=-1)
                all_probs.append(probs.cpu().numpy())

        return np.vstack(all_probs)

# ==== 2. Student Helpers ====
def create_student(random_state: int, max_features: int, alpha: float = 1.0, model_type="ridge"):
    """Creates a TF-IDF student. S1=Ridge, S2=SGD for diversity."""
    if model_type == "sgd":
        # SGD Regressor acts as the 'adversarial' view
        reg = MultiOutputRegressor(SGDRegressor(loss="squared_error", random_state=random_state))
    else:
        # Ridge is the standard stable solver
        reg = MultiOutputRegressor(Ridge(alpha=alpha, random_state=random_state))

    return Pipeline([
        ("tfidf", TfidfVectorizer(ngram_range=(1, 2), max_features=max_features, min_df=2, stop_words="english")),
        ("reg", reg)
    ])

def calculate_disagreement(p1, p2):
    """Returns L2 distance squared between prediction vectors."""
    return np.sum((p1 - p2)**2, axis=1)

# ==== 3. Main Execution ====

# A. Load Data
print(f"[data] Loading dataset {DATASET_NAME}...")
ds = load_dataset(DATASET_NAME, split="test") # Using 'test' split for speed/size
texts = np.array(ds["text"])

# Shuffle indices
indices = np.arange(len(texts))
rng = np.random.RandomState(42)
rng.shuffle(indices)

# B. Split: Validation vs Pool
val_indices = indices[:VAL_SIZE]
pool_indices = indices[VAL_SIZE:]

X_val = texts[val_indices]

# C. Initialize Teacher
teacher = LiveTeacher(TEACHER_MODEL_NAME, device=DEVICE)

# Get Ground Truth for Validation (Expensive but needed for metrics)
print("[setup] Querying teacher for validation set labels...")
Y_val = teacher.predict(X_val.tolist())
teacher_top1_val = Y_val.argmax(axis=1)

# D. Active Learning Initialization
labeled_indices = pool_indices[:INITIAL_SIZE]
unlabeled_indices = pool_indices[INITIAL_SIZE:]

# Query initial seed set
X_train = texts[labeled_indices]
print(f"[setup] Querying teacher for initial {len(X_train)} seed samples...")
Y_train = teacher.predict(X_train.tolist())

# Create Two Diverse Students
# S1: Ridge (Standard)
s1 = create_student(random_state=42, max_features=50000, model_type="ridge")
# S2: SGD (Adversarial/Different)
s2 = create_student(random_state=43, max_features=50000, model_type="sgd")

print("-" * 80)
print(f"{'Round':<5} | {'Queries':<7} | {'Pool Left':<9} | {'Acc (S1)':<10} | {'Avg Disagreement':<20}")
print("-" * 80)

curr_queries = len(labeled_indices)

# E. Active Learning Loop
while curr_queries < QUERY_BUDGET:
    # 1. Train Both Students
    s1.fit(X_train, Y_train)
    s2.fit(X_train, Y_train)

    # 2. Evaluate S1
    p_val = s1.predict(X_val)
    p_val = np.clip(p_val, 0, 1) # simple clip for safety
    acc = accuracy_score(teacher_top1_val, p_val.argmax(axis=1))

    # 3. DisGUIDE Selection Strategy
    # Optimization: Predict on a random subset of unlabeled pool (e.g. 2000) to save time
    # rather than all 30k+ samples every round.
    candidate_limit = 2000
    if len(unlabeled_indices) > candidate_limit:
        candidate_subset_idx = unlabeled_indices[:candidate_limit]
    else:
        candidate_subset_idx = unlabeled_indices

    X_candidates = texts[candidate_subset_idx]

    # Get predictions from both students
    p1 = s1.predict(X_candidates)
    p2 = s2.predict(X_candidates)

    # Calculate Disagreement
    scores = calculate_disagreement(p1, p2)
    avg_disagreement = np.mean(scores)

    # Select Top-K highest disagreement
    k = min(STEP_SIZE, QUERY_BUDGET - curr_queries)

    # Argsort is ascending, so we take the last k
    top_k_local = np.argsort(scores)[-k:]

    # Map back to global text array
    selected_indices_global = candidate_subset_idx[top_k_local]

    # 4. Query the Real Teacher
    X_new = texts[selected_indices_global]
    Y_new = teacher.predict(X_new.tolist())

    # 5. Update Training Data
    X_train = np.concatenate([X_train, X_new])
    Y_train = np.concatenate([Y_train, Y_new])

    # Update Indices (Remove selected from unlabeled)
    # Fast numpy set difference to remove selected indices
    unlabeled_indices = np.setdiff1d(unlabeled_indices, selected_indices_global)

    print(f"{curr_queries:<5} | {len(X_train):<7} | {len(unlabeled_indices):<9} | {acc:.4f}     | {avg_disagreement:.4f}")

    curr_queries += k

print("-" * 80)
print("[done] Final training round...")
s1.fit(X_train, Y_train)
final_acc = accuracy_score(teacher_top1_val, s1.predict(X_val).argmax(axis=1))
print(f"[result] Final Student Accuracy: {final_acc:.4f}")

# Save Model
dump(s1, "student_live_disguide.joblib")
print("[save] Model saved to student_live_disguide.joblib")